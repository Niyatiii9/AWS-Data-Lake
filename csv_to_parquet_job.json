{
	"jobConfig": {
		"name": "csv_to_parquet_job",
		"description": "",
		"role": "arn:aws:iam::971422715328:role/AWSGlueServiceRole-retail",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "csv_to_parquet_job.py",
		"scriptLocation": "s3://aws-glue-assets-971422715328-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-03-20T06:00:33.321Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-971422715328-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-971422715328-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.functions import col, year, month, to_date\nfrom pyspark.sql.types import IntegerType, DoubleType\nfrom awsglue.dynamicframe import DynamicFrame\n\n# ✅ Initialize Spark & Glue Context\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(\"csv_to_parquet_partitioned\", getResolvedOptions(sys.argv, ['JOB_NAME']))\n\n# ✅ Define input S3 path\ninput_s3_path = \"s3://sale-data-lake/raw/sale-data-lake.csv\"\n\n# ✅ Read raw CSV data from S3\ndf = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\"paths\": [input_s3_path]},\n    format=\"csv\",\n    format_options={\"withHeader\": True}\n)\n\n# ✅ Convert DynamicFrame to DataFrame\ndf_spark = df.toDF()\n\n# ✅ Convert order_date to DATE format\ndf_spark = df_spark.withColumn(\"order_date\", to_date(col(\"order_date\"), \"M/d/yy\"))\n\n# ✅ Extract order_year and order_month for partitioning\ndf_spark = df_spark.withColumn(\"order_year\", year(col(\"order_date\")))\ndf_spark = df_spark.withColumn(\"order_month\", month(col(\"order_date\")))\n\n# ✅ Fix Data Types:\ndf_spark = df_spark.withColumn(\"price\", col(\"price\").cast(DoubleType()))   # Ensure price is DOUBLE\ndf_spark = df_spark.withColumn(\"quantity\", col(\"quantity\").cast(IntegerType()))  # ✅ Fix: Ensure quantity is INTEGER\n\n# ✅ Convert DataFrame back to DynamicFrame\ndf_transformed = DynamicFrame.fromDF(df_spark, glueContext)\n\n# ✅ Define output S3 path\noutput_s3_path = \"s3://sale-data-lake/processed/\"\n\n# ✅ Write partitioned data as Parquet to S3\nglueContext.write_dynamic_frame.from_options(\n    frame=df_transformed,\n    connection_type=\"s3\",\n    connection_options={\n        \"path\": output_s3_path,\n        \"partitionKeys\": [\"order_year\", \"order_month\"]\n    },\n    format=\"parquet\"\n)\n\n# ✅ Commit the Glue job\njob.commit()\n"
}